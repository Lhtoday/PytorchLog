{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### 3.1 Tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [
    {
     "data": {
      "text/plain": "'1.13.1'"
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from __future__ import print_function\n",
    "import torch as t\n",
    "t.__version__"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "###### 从接口的角度来讲, 对tensor的操作分为两类\n",
    "> torch.function   如 torch.save等\n",
    "> 另一类是 tensor.function 如 tensor.view\n",
    "###### 一般来说 tensor的大部分操作同时支持这两类接口 如torch.sum (torch.sum(a, b))与tensor.sum (a.sum(b))功能等价\n",
    "###### 从存储的角度来说, 对tensor的操作可分为两类\n",
    "> 不会修改自身的数据 如 a.add(b) 加法的结果会返回一个新的tensor\n",
    ">   会修改自身的数据 如 a.add_(b) 假发的结果在 a中 a被修改\n",
    "函数名用 _ 位结尾都是 inplace 方式 就是会修改自身的数据 要注意"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "|     函数     | 功能 |\n",
    "|:----------:|:--:|\n",
    "| eye(*size) | 对角线为1, 其他是0 |\n",
    "|arange(s, e, step)|从s到e步长为step|\n",
    "|linspace(s, e, step)|从s到e， 均匀分成step份|\n",
    "|rand/randn(*size)|均匀/标准分布|\n",
    "|normal(mean, std)/uniform(from, to)|正态分布/均匀分布|\n",
    "|randperm(m)|随机排列|"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "###### Tensor 能够接收一个 list 并根据list来创建tensor, 也能根据指定的形状创建tensor\n",
    "###### 还能传入其他的tensor"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [
    {
     "data": {
      "text/plain": "tensor([[9.3582e+25, 5.2268e-43, 9.3582e+25],\n        [5.2268e-43, 9.3582e+25, 5.2268e-43],\n        [9.3582e+25, 5.2268e-43, 9.3582e+25],\n        [5.2268e-43, 9.3582e+25, 5.2268e-43],\n        [9.3582e+25, 5.2268e-43, 9.3582e+25]])"
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#指定tensor形状\n",
    "a = t.Tensor(5, 3)\n",
    "a #数值取决于内存空间的状态 print的时候可能内存溢出(overflow)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [
    {
     "data": {
      "text/plain": "tensor([[2., 3., 1.],\n        [6., 4., 5.]])"
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#用list来创建tensor\n",
    "b = t.Tensor([[2, 3, 1], [6, 4, 5]])\n",
    "b"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [
    {
     "data": {
      "text/plain": "(tensor([[2., 3., 1.],\n         [6., 4., 5.]]),\n [[2.0, 3.0, 1.0], [6.0, 4.0, 5.0]])"
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cc = b.tolist()\n",
    "b, cc"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [
    {
     "data": {
      "text/plain": "torch.Size([2, 3])"
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b_size = b.size()\n",
    "b_size"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [
    {
     "data": {
      "text/plain": "6"
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b.numel()#b中元素个数"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [
    {
     "data": {
      "text/plain": "(tensor([[1.4586e-19, 1.1578e+27, 2.0780e-07],\n         [6.0542e+22, 7.8675e+34, 4.6894e+27]]),\n tensor([2., 3.]))"
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#创建一个和b形状一致的tensor\n",
    "c = t.Tensor(b_size)\n",
    "d = t.Tensor((2, 3))\n",
    "c, d"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "###### tensor.shape 等价于 tensor.size()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [
    {
     "data": {
      "text/plain": "torch.Size([2, 3])"
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "c.shape"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "###### t.Tensor在创建tensor时, 系统不会马上分配空间, 只是计算剩余的内存是否足够使用\n",
    "###### 使用的时候才分配, 其他方法都是创建王就立刻分配"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [
    {
     "data": {
      "text/plain": "tensor([[1., 1.],\n        [1., 1.],\n        [1., 1.]])"
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t.ones(3, 2)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [
    {
     "data": {
      "text/plain": "tensor([[0., 0., 0.],\n        [0., 0., 0.]])"
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t.zeros(2, 3)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [
    {
     "data": {
      "text/plain": "tensor([3, 6])"
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t.arange(3, 8, 3)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "outputs": [
    {
     "data": {
      "text/plain": "tensor([2.0000, 4.3333, 6.6667, 9.0000])"
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t.linspace(2, 9, 4)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "outputs": [
    {
     "data": {
      "text/plain": "tensor([[-0.0137, -3.1169,  0.4358],\n        [ 0.0688, -0.9607, -1.1864]])"
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t.randn(2, 3, device=t.device('cpu'))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "outputs": [
    {
     "data": {
      "text/plain": "tensor([1, 3, 5, 0, 2, 4])"
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t.randperm(6) #长度是6的随机排列"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "outputs": [
    {
     "data": {
      "text/plain": "tensor([[1, 0, 0],\n        [0, 1, 0]])"
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t.eye(2, 3, dtype=int)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "scalar : tensor(3.1416), shape of scalar : torch.Size([])\n"
     ]
    },
    {
     "data": {
      "text/plain": "torch.Size([])"
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scalar = t.tensor(3.14159)\n",
    "print('scalar : %s, shape of scalar : %s' %(scalar, scalar.size()))\n",
    "scalar.shape"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vec : tensor([2, 3, 4]), shape of vec : torch.Size([3])\n"
     ]
    }
   ],
   "source": [
    "vec = t.tensor([2, 3, 4])\n",
    "print('vec : %s, shape of vec : %s' %(vec, vec.shape))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vec2 : tensor([2., 3., 4.]), shape of vec2 : torch.Size([3])\n"
     ]
    }
   ],
   "source": [
    "vec2 = t.Tensor([2, 3, 4])\n",
    "print('vec2 : %s, shape of vec2 : %s' %(vec2, vec2.shape))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "tensor() takes 1 positional argument but 3 were given",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mTypeError\u001B[0m                                 Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[20], line 1\u001B[0m\n\u001B[1;32m----> 1\u001B[0m vec3 \u001B[38;5;241m=\u001B[39m \u001B[43mt\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mtensor\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m2\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m3\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m4\u001B[39;49m\u001B[43m)\u001B[49m\n\u001B[0;32m      2\u001B[0m \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mvec3 : \u001B[39m\u001B[38;5;132;01m%s\u001B[39;00m\u001B[38;5;124m, shape of vec3 : \u001B[39m\u001B[38;5;132;01m%s\u001B[39;00m\u001B[38;5;124m'\u001B[39m \u001B[38;5;241m%\u001B[39m(vec3, vec3\u001B[38;5;241m.\u001B[39mshape))\n",
      "\u001B[1;31mTypeError\u001B[0m: tensor() takes 1 positional argument but 3 were given"
     ]
    }
   ],
   "source": [
    "vec3 = t.tensor(2, 3, 4)\n",
    "print('vec3 : %s, shape of vec3 : %s' %(vec3, vec3.shape))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "vec4 = t.Tensor(2, 3, 4)\n",
    "print('vec4 : %s, shape of vec4 : %s' %(vec4, vec4.shape))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "matrix = t.tensor([[0.1, 1.2], [2.4, 3.7], [6., 7.7]])\n",
    "matrix, matrix.shape"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "tt = t.tensor([[0.11111, 0.222222, 0.3333333]],\n",
    "                     dtype=t.float64,\n",
    "                     device=t.device('cpu'))\n",
    "tt, tt.shape"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "empty_tensor = t.tensor([])\n",
    "empty_tensor.shape"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 常用Tensor操作"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "###### 通过tensor.view 可以调整tensor的形状, 但是必须保持调整前后的元素总数一直。view不会\n",
    "###### 修改自身的数据, 返回的新的tensor和原tensor共享内存, 改变其中一个另一个也会发生改变\n",
    "###### 实际应用中可能经常需要添加或者减少某一维度, 这就要用到squeeze 和 unsqueeze"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "a = t.arange(1, 9) #不包含9 即 [1, 9)左闭右开\n",
    "a.view(4, 2)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "shape '[2, -1]' is invalid for input of size 15",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mRuntimeError\u001B[0m                              Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[21], line 1\u001B[0m\n\u001B[1;32m----> 1\u001B[0m b \u001B[38;5;241m=\u001B[39m \u001B[43ma\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mview\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m2\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m-\u001B[39;49m\u001B[38;5;241;43m1\u001B[39;49m\u001B[43m)\u001B[49m \u001B[38;5;66;03m#有-1的时候会自动计算维度\u001B[39;00m\n\u001B[0;32m      2\u001B[0m b, b\u001B[38;5;241m.\u001B[39mshape\n",
      "\u001B[1;31mRuntimeError\u001B[0m: shape '[2, -1]' is invalid for input of size 15"
     ]
    }
   ],
   "source": [
    "b = a.view(2, -1) #有-1的时候会自动计算维度\n",
    "b, b.shape"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "us1 = b.unsqueeze(1)\n",
    "us1, us1.shape"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "us2 = b.unsqueeze(0)\n",
    "us2, us2.shape"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "us3 = b.unsqueeze(2)\n",
    "us3, us3.shape"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "##### unsqueeze(idx) 在第几维增加一维\n",
    "##### squeeze(idx) 在第几维减少一维"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "outputs": [
    {
     "data": {
      "text/plain": "(tensor([[[[[0, 1, 2, 3],\n            [4, 5, 6, 7]]]]]),\n tensor([[[[0, 1, 2, 3],\n           [4, 5, 6, 7]]]]))"
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "st = t.arange(0, 8)\n",
    "sa = st.view(1, 1, 1, 2, 4)\n",
    "sb = sa.squeeze(0)\n",
    "sa, sb"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "outputs": [
    {
     "data": {
      "text/plain": "(tensor([[1., 2.],\n         [3., 4.],\n         [1., 2.],\n         [3., 4.]]),\n torch.Size([4, 2]),\n tensor([[1., 2.],\n         [3., 4.],\n         [1., 2.],\n         [3., 4.]]),\n torch.Size([4, 2]))"
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sst = t.Tensor([[1, 2, 3, 4], [1, 2, 3, 4]])\n",
    "ssa = sst.view(4, 2)\n",
    "ssb = t.squeeze(ssa, 0)\n",
    "ssa, ssa.shape, ssb, ssb.shape"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "outputs": [],
   "source": [
    "sst[0][0] = 100"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "outputs": [
    {
     "data": {
      "text/plain": "(tensor([[100.,   2.],\n         [  3.,   4.],\n         [  1.,   2.],\n         [  3.,   4.]]),\n tensor([[100.,   2.],\n         [  3.,   4.],\n         [  1.,   2.],\n         [  3.,   4.]]))"
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ssa, ssb"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "###### resize 也可以重新分配大小，不过区别于view，当元素数目不满足分为新的唯独所需要的数目\n",
    "###### 会报错， 而 resize 不会， 当新的维度的元素数目少于 原来的维度元素数目 就会\n",
    "###### 截取一部分 而当新的维度的元素数目大于 原来的维度元素数目 就会分配额外空间"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "outputs": [
    {
     "data": {
      "text/plain": "tensor([0, 1, 2, 3, 4, 5, 6, 7])"
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ttt = t.arange(0, 8)\n",
    "ttt"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "outputs": [
    {
     "data": {
      "text/plain": "tensor([[0, 1, 2, 3],\n        [4, 5, 6, 7]])"
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ttta = ttt.view([2, 4])\n",
    "ttta"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\liuhao\\.conda\\envs\\PyTorch\\lib\\site-packages\\torch\\_tensor.py:761: UserWarning: non-inplace resize is deprecated\n",
      "  warnings.warn(\"non-inplace resize is deprecated\")\n"
     ]
    },
    {
     "data": {
      "text/plain": "tensor([[0, 1],\n        [2, 3],\n        [4, 5],\n        [6, 7]])"
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tttc = ttta.resize(4, 2)\n",
    "tttc"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "requested resize to 3x3 (9 elements in total), but the given tensor has a size of 2x4 (8 elements). autograd's resize can only change the shape of a given tensor, while preserving the number of elements. ",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mRuntimeError\u001B[0m                              Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[29], line 1\u001B[0m\n\u001B[1;32m----> 1\u001B[0m tttd \u001B[38;5;241m=\u001B[39m \u001B[43mttta\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mresize\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m3\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m3\u001B[39;49m\u001B[43m)\u001B[49m\n\u001B[0;32m      2\u001B[0m tttd\n",
      "File \u001B[1;32m~\\.conda\\envs\\PyTorch\\lib\\site-packages\\torch\\_tensor.py:764\u001B[0m, in \u001B[0;36mTensor.resize\u001B[1;34m(self, *sizes)\u001B[0m\n\u001B[0;32m    761\u001B[0m warnings\u001B[38;5;241m.\u001B[39mwarn(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mnon-inplace resize is deprecated\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[0;32m    762\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mtorch\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mautograd\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01m_functions\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m Resize\n\u001B[1;32m--> 764\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mResize\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mapply\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43msizes\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32m~\\.conda\\envs\\PyTorch\\lib\\site-packages\\torch\\autograd\\_functions\\tensor.py:32\u001B[0m, in \u001B[0;36mResize.forward\u001B[1;34m(ctx, tensor, sizes)\u001B[0m\n\u001B[0;32m     30\u001B[0m ctx\u001B[38;5;241m.\u001B[39mnumel \u001B[38;5;241m=\u001B[39m reduce(\u001B[38;5;28;01mlambda\u001B[39;00m x, y: x \u001B[38;5;241m*\u001B[39m y, sizes, \u001B[38;5;241m1\u001B[39m)\n\u001B[0;32m     31\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m tensor\u001B[38;5;241m.\u001B[39mnumel() \u001B[38;5;241m!=\u001B[39m ctx\u001B[38;5;241m.\u001B[39mnumel:\n\u001B[1;32m---> 32\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mRuntimeError\u001B[39;00m((\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mrequested resize to \u001B[39m\u001B[38;5;132;01m{}\u001B[39;00m\u001B[38;5;124m (\u001B[39m\u001B[38;5;132;01m{}\u001B[39;00m\u001B[38;5;124m elements in total), \u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m     33\u001B[0m                         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mbut the given tensor has a size of \u001B[39m\u001B[38;5;132;01m{}\u001B[39;00m\u001B[38;5;124m (\u001B[39m\u001B[38;5;132;01m{}\u001B[39;00m\u001B[38;5;124m elements). \u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m     34\u001B[0m                         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mautograd\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124ms resize can only change the shape of a given \u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m     35\u001B[0m                         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mtensor, while preserving the number of elements. \u001B[39m\u001B[38;5;124m\"\u001B[39m)\u001B[38;5;241m.\u001B[39mformat(\n\u001B[0;32m     36\u001B[0m         \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mx\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;241m.\u001B[39mjoin(\u001B[38;5;28mmap\u001B[39m(\u001B[38;5;28mstr\u001B[39m, sizes)), ctx\u001B[38;5;241m.\u001B[39mnumel,\n\u001B[0;32m     37\u001B[0m         \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mx\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;241m.\u001B[39mjoin(\u001B[38;5;28mmap\u001B[39m(\u001B[38;5;28mstr\u001B[39m, tensor\u001B[38;5;241m.\u001B[39msize())), tensor\u001B[38;5;241m.\u001B[39mnumel()))\n\u001B[0;32m     38\u001B[0m ctx\u001B[38;5;241m.\u001B[39minput_sizes \u001B[38;5;241m=\u001B[39m tensor\u001B[38;5;241m.\u001B[39msize()\n\u001B[0;32m     39\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m tensor\u001B[38;5;241m.\u001B[39mis_quantized:\n",
      "\u001B[1;31mRuntimeError\u001B[0m: requested resize to 3x3 (9 elements in total), but the given tensor has a size of 2x4 (8 elements). autograd's resize can only change the shape of a given tensor, while preserving the number of elements. "
     ]
    }
   ],
   "source": [
    "tttd = ttta.resize(3, 3)\n",
    "tttd"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "###### 看来resize和view没什么区别 resize不能填充或者删除元素"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "outputs": [
    {
     "data": {
      "text/plain": "tensor([[                  0,                   1,                   2],\n        [                  3,                   4,                   5],\n        [                  6,                   7, 7308535291872879145]])"
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ttta.resize_(3, 3)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "outputs": [
    {
     "data": {
      "text/plain": "tensor([[0, 1]])"
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ttta.resize_(1, 2)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### resize_ 行！！！"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### 索引操作\n",
    "###### 类似numpy.ndarray"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "outputs": [
    {
     "data": {
      "text/plain": "tensor([[-1.0166, -0.9107, -0.4300,  0.4374],\n        [-0.6738,  0.6675,  1.4845, -0.6912],\n        [ 1.5548,  0.3883,  0.7748, -0.8407]])"
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = t.randn(3, 4)\n",
    "a"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "outputs": [
    {
     "data": {
      "text/plain": "(tensor([[100.0000, 100.0000, 100.0000, 100.0000],\n         [ -0.6738,   0.6675,   1.4845,  -0.6912],\n         [  1.5548,   0.3883,   0.7748,  -0.8407]]),\n tensor([100., 100., 100., 100.]))"
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b = a[0]\n",
    "a[0] = 100\n",
    "a, b #a b共享内存"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "outputs": [
    {
     "data": {
      "text/plain": "(tensor([[[100.0000, 100.0000, 100.0000, 100.0000],\n          [ -0.6738,   0.6675,   1.4845,  -0.6912],\n          [  1.5548,   0.3883,   0.7748,  -0.8407]]]),\n torch.Size([1, 3, 4]))"
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "c = a[None]\n",
    "c, c.shape#None 相当于 view(1, a.shape[0], a.shape[1])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "outputs": [
    {
     "data": {
      "text/plain": "(torch.Size([3, 1, 4]), torch.Size([3, 1, 4, 1, 1]))"
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "d = a[:, None, :]\n",
    "e = a[:, None, :, None, None]\n",
    "d.shape, e.shape"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "outputs": [
    {
     "data": {
      "text/plain": "tensor([[ True,  True,  True,  True],\n        [False, False,  True, False],\n        [ True, False, False, False]])"
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a > 1"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "outputs": [
    {
     "data": {
      "text/plain": "(tensor([100.0000,   0.0000, 100.0000, 100.0000,   1.4845,   1.5548]),\n tensor([[100.0000, 100.0000, 100.0000, 100.0000],\n         [ -0.6738,   0.6675,   1.4845,  -0.6912],\n         [  1.5548,   0.3883,   0.7748,  -0.8407]]))"
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f = a[a > 1]\n",
    "f[1] = 0\n",
    "f, a\n",
    "#选择结果 不共享内存 相当于 masked_select(a > 1)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "outputs": [
    {
     "data": {
      "text/plain": "tensor([[ 1,  2,  3,  4],\n        [ 5,  6,  7,  8],\n        [ 9, 10, 11, 12],\n        [13, 14, 15, 16]])"
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = t.arange(1, 17).view(4, 4)\n",
    "a"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "outputs": [
    {
     "data": {
      "text/plain": "tensor([[13, 10,  7,  4]])"
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "index = t.LongTensor([[3, 2, 1, 0]])\n",
    "#(0, 0) (0, 1) (0, 2) (0, 3)\n",
    "#dim = 0, 1\n",
    "#dim = 0 就是用index替换坐标的第一位\n",
    "#比如 (3, 0) (2, 1) (1, 2) (0, 3)\n",
    "#答案是 13, 10, 7, 4\n",
    "b = a.gather(0, index)\n",
    "b"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "outputs": [
    {
     "data": {
      "text/plain": "torch.Size([1, 4])"
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b.shape"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "outputs": [
    {
     "data": {
      "text/plain": "(tensor([[ 1,  2,  3,  4],\n         [ 5,  6,  7,  8],\n         [ 9, 10, 11, 12],\n         [13, 14, 15, 16]]),\n tensor([[13, 10, -1,  4]]))"
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b[0][2] = -1\n",
    "a, b #不共享内存"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "outputs": [
    {
     "data": {
      "text/plain": "tensor([[0, 3],\n        [1, 2],\n        [2, 1],\n        [3, 0]])"
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "index = t.LongTensor([[0, 1, 2, 3], [3, 2, 1, 0]]).t()\n",
    "index"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "outputs": [
    {
     "data": {
      "text/plain": "tensor([[ 1, 14],\n        [ 5, 10],\n        [ 9,  6],\n        [13,  2]])"
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b = a.gather(0, index)\n",
    "b"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "outputs": [
    {
     "data": {
      "text/plain": "tensor([[0., 0., 0., 0.],\n        [0., 0., 0., 0.],\n        [0., 0., 0., 0.],\n        [0., 0., 0., 0.]])"
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "c = t.zeros(16).view(4, 4)\n",
    "c"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "outputs": [
    {
     "data": {
      "text/plain": "tensor([[ 1.,  2.,  0., 14.],\n        [ 5.,  5., 10.,  0.],\n        [ 9.,  6.,  9.,  0.],\n        [ 2., 14.,  0., 13.]])"
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "c.scatter_(1, index, b.float())\n",
    "c"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}